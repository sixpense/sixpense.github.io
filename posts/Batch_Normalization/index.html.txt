---
Title: Batch Normalization
Date: 2020-05-21
Author: sixpense
---

# Batch Normalization原理

## 1. 炼丹的困扰

在深度学习中, 我们往往会使用较深的层数的网络进行学习, 在这个过程中, 不同的的学
习率的参数方法, 可以帮助我们的模型加速收敛. 深度神经网络之所以如此难收敛, 很大
一个原因就是网络层与层中存在高度的相关性和耦合性.

随着训练的进行, 网络中的参数也随着梯度下降在不停更新. 一方面, 当底层网络中参数
发生微弱变化时, 由于每一层中的线性变换与非线性激活映射, 这些微弱变化随着网络层
数的加深而被放大(类似蝴蝶效应), 另一方面, 参数的变化导致每一层的输入分布会发
生改变, 进而上层的网络需要不停地去适应这些分布变化, 使得我们的模型训练变得困难
. 上述这一现象叫做Internal Covariate Shift.

图片插入
@[./book-reading.jpg]

