---
title: Batch Normalization
Date: 2020-05-21
Author: sixpense
---

# Batch Normalization原理

## 1. 炼丹的困扰

在深度学习中, 我们往往会使用较深的层数的网络进行学习, 在这个过程中, 不同的的学
习率的参数方法, 可以帮助我们的模型加速收敛. 深度神经网络之所以如此难收敛, 很大
一个原因就是网络层与层中存在高度的相关性和耦合性.
